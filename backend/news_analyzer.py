import aiohttp
import asyncio
import feedparser
from datetime import datetime, timedelta
import pytz
import re
from typing import List, Dict, Any
from config import Config

logger = logging.getLogger("news_analyzer")

class NewsAnalyzer:
    def __init__(self):
        self.kyiv_tz = pytz.timezone('Europe/Kiev')
        self.session = None
        
        # –°–ª–æ–≤–Ω–∏–∫ –¥–ª—è –ø–µ—Ä–µ–∫–ª–∞–¥—É –¥–Ω—ñ–≤/–º—ñ—Å—è—Ü—ñ–≤ –≤ RSS
        self.ukrainian_months = {
            '—Å—ñ—á–Ω—è': '01', '–ª—é—Ç–æ–≥–æ': '02', '–±–µ—Ä–µ–∑–Ω—è': '03', '–∫–≤—ñ—Ç–Ω—è': '04',
            '—Ç—Ä–∞–≤–Ω—è': '05', '—á–µ—Ä–≤–Ω—è': '06', '–ª–∏–ø–Ω—è': '07', '—Å–µ—Ä–ø–Ω—è': '08',
            '–≤–µ—Ä–µ—Å–Ω—è': '09', '–∂–æ–≤—Ç–Ω—è': '10', '–ª–∏—Å—Ç–æ–ø–∞–¥–∞': '11', '–≥—Ä—É–¥–Ω—è': '12'
        }
        
        # –î–µ—Ç–∞–ª—å–Ω—ñ—à—ñ –∫–ª—é—á–æ–≤—ñ —Å–ª–æ–≤–∞ –¥–ª—è –∫—Ä–∞—â–æ–≥–æ –∞–Ω–∞–ª—ñ–∑—É
        self.keyword_groups = {
            'positive_market': ['–∑—Ä–æ—Å—Ç–∞–Ω–Ω—è', '–ø—ñ–¥–≤–∏—â–µ–Ω–Ω—è', '–ø—Ä–∏–±—É—Ç–æ–∫', '—ñ–Ω–≤–µ—Å—Ç–∏—Ü—ñ—ó', '—Ä–æ–∑–≤–∏—Ç–æ–∫',
                               '–ø–æ–∫—Ä–∞—â–µ–Ω–Ω—è', '—Å—Ç–∞–±—ñ–ª—å–Ω—ñ—Å—Ç—å', '–ø–æ–∑–∏—Ç–∏–≤–Ω–∏–π', '—Å–∏–ª—å–Ω–∏–π', '—Ä–µ–∫–æ—Ä–¥'],
            
            'negative_market': ['—Å–ø–∞–¥', '–ø–∞–¥—ñ–Ω–Ω—è', '–∑–Ω–∏–∂–µ–Ω–Ω—è', '–∑–±–∏—Ç–∫–∏', '–∫—Ä–∏–∑–∞', '–Ω–µ—Å—Ç–∞–±—ñ–ª—å–Ω—ñ—Å—Ç—å',
                               '–Ω–µ–≥–∞—Ç–∏–≤–Ω–∏–π', '—Å–ª–∞–±–∫–∏–π', '–æ–±–º–µ–∂–µ–Ω–Ω—è', '–¥–µ—Ñ—ñ—Ü–∏—Ç', '—ñ–º–ø—ñ—á–º–µ–Ω—Ç'],
            
            'inflation': ['—ñ–Ω—Ñ–ª—è—Ü—ñ—è', '—ñ–Ω—Ñ–ª—è—Ü—ñ–π–Ω–∏–π', '—Ü—ñ–Ω–∏', '–ø—ñ–¥–≤–∏—â–µ–Ω–Ω—è —Ü—ñ–Ω', '–∑—Ä–æ—Å—Ç–∞–Ω–Ω—è —Ü—ñ–Ω'],
            
            'interest_rates': ['–≤—ñ–¥—Å–æ—Ç–∫–æ–≤–∞ —Å—Ç–∞–≤–∫–∞', '–∫–ª—é—á–æ–≤–∞ —Å—Ç–∞–≤–∫–∞', '—Å—Ç–∞–≤–∫–∞ –ù–ë–£', '—Å—Ç–∞–≤–∫–∞ –§–†–°'],
            
            'geopolitical': ['–≤—ñ–π–Ω–∞', '–∫–æ–Ω—Ñ–ª—ñ–∫—Ç', '—Å–∞–Ω–∫—Ü—ñ—ó', '–ø–µ—Ä–µ–≥–æ–≤–æ—Ä–∏', '—É–≥–æ–¥–∞', '–º–∏—Ä'],
            
            'economic_data': ['–í–í–ü', '–µ–∫–æ–Ω–æ–º—ñ—á–Ω–µ –∑—Ä–æ—Å—Ç–∞–Ω–Ω—è', '–±–µ–∑—Ä–æ–±—ñ—Ç—Ç—è', '–µ–∫—Å–ø–æ—Ä—Ç', '—ñ–º–ø–æ—Ä—Ç']
        }

    async def get_latest_news(self, hours_back: int = 24, min_news_count: int = 10) -> List[Dict[str, Any]]:
        """–û—Ç—Ä–∏–º–∞—Ç–∏ –æ—Å—Ç–∞–Ω–Ω—ñ –Ω–æ–≤–∏–Ω–∏ –∑ —Ä—ñ–∑–Ω–∏—Ö –¥–∂–µ—Ä–µ–ª"""
        logger.info(f"üì∞ –û—Ç—Ä–∏–º–∞–Ω–Ω—è –Ω–æ–≤–∏–Ω –∑–∞ –æ—Å—Ç–∞–Ω–Ω—ñ {hours_back} –≥–æ–¥–∏–Ω...")
        
        all_news = []
        
        # –°—Ç–≤–æ—Ä—é—î–º–æ —Å–µ—Å—ñ—é aiohttp
        async with aiohttp.ClientSession() as session:
            self.session = session
            
            # –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ –æ—Ç—Ä–∏–º—É—î–º–æ –Ω–æ–≤–∏–Ω–∏ –∑ —É—Å—ñ—Ö –¥–∂–µ—Ä–µ–ª
            tasks = []
            for source in Config.NEWS_SOURCES:
                if source['type'] == 'rss':
                    tasks.append(self._fetch_rss_news(source, hours_back))
                elif source['type'] == 'api':
                    if source.get('requires_key', False) and not Config.NEWS_API_KEY:
                        continue
                    tasks.append(self._fetch_api_news(source, hours_back))
            
            # –í–∏–∫–æ–Ω—É—î–º–æ –≤—Å—ñ –∑–∞–ø–∏—Ç–∏ –ø–∞—Ä–∞–ª–µ–ª—å–Ω–æ
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            # –û–±'—î–¥–Ω—É—î–º–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏
            for result in results:
                if isinstance(result, list):
                    all_news.extend(result)
        
        # –§—ñ–ª—å—Ç—Ä—É—î–º–æ –¥—É–±–ª—ñ–∫–∞—Ç–∏
        unique_news = self._remove_duplicates(all_news)
        
        # –°–æ—Ä—Ç—É—î–º–æ –∑–∞ –¥–∞—Ç–æ—é (–Ω–æ–≤—ñ—à—ñ –ø–µ—Ä—à—ñ)
        unique_news.sort(key=lambda x: x.get('published_timestamp', ''), reverse=True)
        
        # –û–±–º–µ–∂—É—î–º–æ –∫—ñ–ª—å–∫—ñ—Å—Ç—å
        news_to_return = unique_news[:50]  # –ë–µ—Ä–µ–º–æ –º–∞–∫—Å–∏–º—É–º 50 –Ω–æ–≤–∏–Ω
        
        logger.info(f"‚úÖ –û—Ç—Ä–∏–º–∞–Ω–æ {len(news_to_return)} —É–Ω—ñ–∫–∞–ª—å–Ω–∏—Ö –Ω–æ–≤–∏–Ω")
        
        if len(news_to_return) < min_news_count:
            logger.warning(f"‚ö†Ô∏è –û—Ç—Ä–∏–º–∞–Ω–æ –∑–∞–º–∞–ª–æ –Ω–æ–≤–∏–Ω ({len(news_to_return)}), –¥–æ–¥–∞—î–º–æ –∫–µ—à–æ–≤–∞–Ω—ñ")
            # –î–æ–¥–∞—î–º–æ –∫–µ—à–æ–≤–∞–Ω—ñ –Ω–æ–≤–∏–Ω–∏ —è–∫—â–æ –ø–æ—Ç—Ä—ñ–±–Ω–æ
            cached = self._get_cached_news()
            if cached:
                news_to_return.extend(cached[:min_news_count - len(news_to_return)])
                news_to_return = news_to_return[:50]
        
        return news_to_return

    async def _fetch_rss_news(self, source: Dict, hours_back: int) -> List[Dict]:
        """–û—Ç—Ä–∏–º–∞—Ç–∏ –Ω–æ–≤–∏–Ω–∏ –∑ RSS –¥–∂–µ—Ä–µ–ª–∞"""
        news_items = []
        
        try:
            # –û—Ç—Ä–∏–º—É—î–º–æ RSS
            async with self.session.get(source['url'], timeout=10) as response:
                if response.status == 200:
                    content = await response.text()
                    
                    # –ü–∞—Ä—Å–∏–º–æ RSS
                    feed = feedparser.parse(content)
                    
                    for entry in feed.entries[:20]:  # –ë–µ—Ä–µ–º–æ 20 –æ—Å—Ç–∞–Ω–Ω—ñ—Ö –∑–∞–ø–∏—Å—ñ–≤
                        try:
                            # –û—Ç—Ä–∏–º—É—î–º–æ —Ç–∞ –ø–∞—Ä—Å–∏–º–æ –¥–∞—Ç—É
                            published_time = self._parse_rss_date(entry.get('published', ''))
                            
                            if not published_time:
                                # –Ø–∫—â–æ –¥–∞—Ç—É –Ω–µ –≤–¥–∞–ª–æ—Å—è —Ä–æ–∑–ø—ñ–∑–Ω–∞—Ç–∏, –±–µ—Ä–µ–º–æ –ø–æ—Ç–æ—á–Ω–∏–π —á–∞—Å
                                published_time = datetime.now(pytz.UTC)
                            
                            # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ, —á–∏ –Ω–æ–≤–∏–Ω–∞ –Ω–µ –∑–∞—Å—Ç–∞—Ä—ñ–ª–∞
                            time_diff = datetime.now(pytz.UTC) - published_time
                            if time_diff <= timedelta(hours=hours_back):
                                
                                # –ê–Ω–∞–ª—ñ–∑—É—î–º–æ –∑–∞–≥–æ–ª–æ–≤–æ–∫ —Ç–∞ –æ–ø–∏—Å
                                title = entry.get('title', '')
                                summary = entry.get('summary', entry.get('description', ''))
                                
                                # –û—á–∏—â–∞—î–º–æ HTML —Ç–µ–≥–∏
                                summary = self._clean_html(summary)
                                
                                # –ê–Ω–∞–ª—ñ–∑—É—î–º–æ —Ç–æ–Ω–∞–ª—å–Ω—ñ—Å—Ç—å
                                sentiment = self._analyze_sentiment(title + ' ' + summary)
                                
                                # –í–∏–∑–Ω–∞—á–∞—î–º–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ñ—Å—Ç—å –¥–ª—è —Ñ—ñ–Ω–∞–Ω—Å—ñ–≤
                                relevance = self._calculate_relevance(title + ' ' + summary)
                                
                                news_item = {
                                    'title': title[:200],
                                    'summary': summary[:500],
                                    'link': entry.get('link', ''),
                                    'published': published_time.isoformat(),
                                    'published_timestamp': published_time.timestamp(),
                                    'source': source['name'],
                                    'source_url': source['url'],
                                    'category': source.get('category', 'general'),
                                    'sentiment': sentiment,
                                    'relevance': relevance,
                                    'has_financial_keywords': relevance > 0,
                                    'id': self._generate_news_id(entry)
                                }
                                
                                # –î–æ–¥–∞—î–º–æ —Ç—ñ–ª—å–∫–∏ —è–∫—â–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ñ—Å—Ç—å > 0 –∞–±–æ —Ç–æ–Ω–∞–ª—å–Ω—ñ—Å—Ç—å –Ω–µ –Ω–µ–π—Ç—Ä–∞–ª—å–Ω–∞
                                if relevance > 0 or sentiment != 'neutral':
                                    news_items.append(news_item)
                                    
                        except Exception as e:
                            logger.debug(f"–ü–æ–º–∏–ª–∫–∞ –æ–±—Ä–æ–±–∫–∏ RSS –∑–∞–ø–∏—Å—É: {e}")
                            continue
                            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è –ü–æ–º–∏–ª–∫–∞ –æ—Ç—Ä–∏–º–∞–Ω–Ω—è RSS –∑ {source['name']}: {e}")
        
        logger.debug(f"üì° {source['name']}: {len(news_items)} –Ω–æ–≤–∏–Ω")
        return news_items

    async def _fetch_api_news(self, source: Dict, hours_back: int) -> List[Dict]:
        """–û—Ç—Ä–∏–º–∞—Ç–∏ –Ω–æ–≤–∏–Ω–∏ —á–µ—Ä–µ–∑ API"""
        # –¢—É—Ç –º–æ–∂–Ω–∞ –¥–æ–¥–∞—Ç–∏ —ñ–Ω—Ç–µ–≥—Ä–∞—Ü—ñ—é –∑ NewsAPI, Alpha Vantage News —Ç–æ—â–æ
        # –ó–∞–≥–ª—É—à–∫–∞ –¥–ª—è –º–∞–π–±—É—Ç–Ω—å–æ—ó —Ä–µ–∞–ª—ñ–∑–∞—Ü—ñ—ó
        return []

    def _parse_rss_date(self, date_str: str):
        """–†–æ–∑–ø—ñ–∑–Ω–∞–≤–∞–Ω–Ω—è –¥–∞—Ç–∏ –∑ —Ä—ñ–∑–Ω–∏—Ö —Ñ–æ—Ä–º–∞—Ç—ñ–≤"""
        if not date_str:
            return None
        
        try:
            # –°–ø—Ä–æ–±—É–≤–∞—Ç–∏ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ñ —Ñ–æ—Ä–º–∞—Ç–∏
            for fmt in [
                '%a, %d %b %Y %H:%M:%S %z',
                '%a, %d %b %Y %H:%M:%S %Z',
                '%Y-%m-%dT%H:%M:%S%z',
                '%Y-%m-%d %H:%M:%S',
                '%d %b %Y %H:%M:%S'
            ]:
                try:
                    dt = datetime.strptime(date_str, fmt)
                    if dt.tzinfo is None:
                        dt = pytz.UTC.localize(dt)
                    return dt
                except:
                    continue
            
            # –°–ø—Ä–æ–±—É–≤–∞—Ç–∏ —É–∫—Ä–∞—ó–Ω—Å—å–∫–∏–π —Ñ–æ—Ä–º–∞—Ç "1 —Å—ñ—á–Ω—è 2024"
            for uk_month, num_month in self.ukrainian_months.items():
                if uk_month in date_str.lower():
                    pattern = r'(\d{1,2})\s+' + uk_month + r'\s+(\d{4})'
                    match = re.search(pattern, date_str.lower())
                    if match:
                        day = match.group(1)
                        year = match.group(2)
                        date_str_iso = f"{year}-{num_month}-{day.zfill(2)}T12:00:00"
                        dt = datetime.strptime(date_str_iso, '%Y-%m-%dT%H:%M:%S')
                        return pytz.UTC.localize(dt)
                        
        except Exception as e:
            logger.debug(f"–ü–æ–º–∏–ª–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥—É –¥–∞—Ç–∏ '{date_str}': {e}")
        
        return None

    def _analyze_sentiment(self, text: str) -> str:
        """–ê–Ω–∞–ª—ñ–∑ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—ñ —Ç–µ–∫—Å—Ç—É"""
        if not text:
            return 'neutral'
        
        text_lower = text.lower()
        
        # –†–∞—Ö—É—î–º–æ –ø–æ–∑–∏—Ç–∏–≤–Ω—ñ —Ç–∞ –Ω–µ–≥–∞—Ç–∏–≤–Ω—ñ –∫–ª—é—á–æ–≤—ñ —Å–ª–æ–≤–∞
        positive_score = 0
        negative_score = 0
        
        for word in self.keyword_groups['positive_market']:
            if word in text_lower:
                positive_score += 1
        
        for word in self.keyword_groups['negative_market']:
            if word in text_lower:
                negative_score += 1
        
        # –î–æ–¥–∞—Ç–∫–æ–≤—ñ –±–∞–ª–∏ –∑–∞ —Å–∏–ª—å–Ω—ñ —Å–ª–æ–≤–∞
        strong_positive = ['—Ä–µ–∫–æ—Ä–¥', '–ø—Ä–æ—Ä–∏–≤', '—ñ—Å—Ç–æ—Ç–Ω–µ –∑—Ä–æ—Å—Ç–∞–Ω–Ω—è', '–∑–Ω–∞—á–Ω–µ –ø–æ–∫—Ä–∞—â–µ–Ω–Ω—è']
        strong_negative = ['–∫—Ä–∏–∑–∞', '–∫—Ä–∞—Ö', '–∫–æ–ª–∞–ø—Å', '–∫–∞—Ç–∞—Å—Ç—Ä–æ—Ñ–∞', '—Ä—É–π–Ω—É–≤–∞–Ω–Ω—è']
        
        for word in strong_positive:
            if word in text_lower:
                positive_score += 2
        
        for word in strong_negative:
            if word in text_lower:
                negative_score += 2
        
        # –í–∏–∑–Ω–∞—á–∞—î–º–æ —Ç–æ–Ω–∞–ª—å–Ω—ñ—Å—Ç—å
        if positive_score == 0 and negative_score == 0:
            return 'neutral'
        elif positive_score > negative_score * 1.5:
            return 'positive'
        elif negative_score > positive_score * 1.5:
            return 'negative'
        else:
            return 'neutral'

    def _calculate_relevance(self, text: str) -> int:
        """–†–æ–∑—Ä–∞—Ö—É–Ω–æ–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—ñ –¥–ª—è —Ñ—ñ–Ω–∞–Ω—Å–æ–≤–æ–≥–æ –∞–Ω–∞–ª—ñ–∑—É"""
        if not text:
            return 0
        
        text_lower = text.lower()
        relevance = 0
        
        # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ –≤—Å—ñ –≥—Ä—É–ø–∏ –∫–ª—é—á–æ–≤–∏—Ö —Å–ª—ñ–≤
        for group_name, keywords in self.keyword_groups.items():
            for keyword in keywords:
                if keyword in text_lower:
                    relevance += 1
        
        # –î–æ–¥–∞—Ç–∫–æ–≤—ñ –±–∞–ª–∏ –∑–∞ –≤–∞–ª—é—Ç–∏ —Ç–∞ –∫—Ä–∏–ø—Ç—É
        for currency in Config.CURRENCIES:
            if currency.lower() in text_lower:
                relevance += 2
        
        for crypto in Config.CRYPTO:
            if crypto.lower() in text_lower:
                relevance += 2
        
        return relevance

    def _clean_html(self, text: str) -> str:
        """–û—á–∏—â–µ–Ω–Ω—è HTML —Ç–µ–≥—ñ–≤ –∑ —Ç–µ–∫—Å—Ç—É"""
        if not text:
            return ''
        
        # –í–∏–¥–∞–ª—è—î–º–æ HTML —Ç–µ–≥–∏
        clean = re.compile('<.*?>')
        text = re.sub(clean, '', text)
        
        # –í–∏–¥–∞–ª—è—î–º–æ —Å–ø–µ—Ü—ñ–∞–ª—å–Ω—ñ —Å–∏–º–≤–æ–ª–∏
        text = text.replace('&nbsp;', ' ').replace('&amp;', '&')
        text = text.replace('&lt;', '<').replace('&gt;', '>')
        text = text.replace('&quot;', '"').replace('&#39;', "'")
        
        # –û–±—Ä—ñ–∑–∞—î–º–æ –∑–∞–π–≤—ñ –ø—Ä–æ–±—ñ–ª–∏
        text = ' '.join(text.split())
        
        return text[:500]  # –û–±–º–µ–∂—É—î–º–æ –¥–æ–≤–∂–∏–Ω—É

    def _remove_duplicates(self, news_list: List[Dict]) -> List[Dict]:
        """–í–∏–¥–∞–ª–µ–Ω–Ω—è –¥—É–±–ª—ñ–∫–∞—Ç—ñ–≤ –Ω–æ–≤–∏–Ω"""
        seen_titles = set()
        unique_news = []
        
        for news in news_list:
            title_key = news.get('title', '').lower()[:100]
            
            if title_key not in seen_titles:
                seen_titles.add(title_key)
                unique_news.append(news)
        
        return unique_news

    def _generate_news_id(self, entry) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü—ñ—è —É–Ω—ñ–∫–∞–ª—å–Ω–æ–≥–æ ID –¥–ª—è –Ω–æ–≤–∏–Ω–∏"""
        import hashlib
        
        title = entry.get('title', '')
        link = entry.get('link', '')
        source = entry.get('source', {}).get('title', '') if hasattr(entry, 'source') else ''
        
        id_string = f"{title}{link}{source}"
        return hashlib.md5(id_string.encode()).hexdigest()[:10]

    def _get_cached_news(self) -> List[Dict]:
        """–û—Ç—Ä–∏–º–∞—Ç–∏ –∫–µ—à–æ–≤–∞–Ω—ñ –Ω–æ–≤–∏–Ω–∏"""
        try:
            cache_file = Config.DATA_DIR / 'news_cache.json'
            if cache_file.exists():
                with open(cache_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    
                    # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ, —á–∏ –∫–µ—à –Ω–µ –∑–∞—Å—Ç–∞—Ä—ñ–≤
                    cache_time_str = data.get('last_update')
                    if cache_time_str:
                        cache_time = datetime.fromisoformat(cache_time_str)
                        if datetime.now(pytz.UTC) - cache_time <= timedelta(hours=Config.CACHE_HOURS):
                            return data.get('news', [])
        except Exception as e:
            logger.debug(f"–ü–æ–º–∏–ª–∫–∞ —á–∏—Ç–∞–Ω–Ω—è –∫–µ—à—É –Ω–æ–≤–∏–Ω: {e}")
        
        return []
